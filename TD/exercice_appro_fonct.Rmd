---
title: "Approximation d’une fonction sinusoïdale par GBM"
author: "Orlana Hashazinka"
output:
  rmdformats::robobook:
    highlight: tango
    css: "style.css"
    self_contained: false
    toc_depth: 3
    code_folding: show
    thumbnails: true
    lightbox: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = 'hide' )
```

## Objectif général

Dans cet exercice, vous allez utiliser un jeu de données simulé pour approximer une fonction sinusoïdale bruitée à l’aide du Gradient Boosting (GBM). L’objectif est de visualiser comment le modèle apprend progressivement la fonction et de comprendre l’impact des hyperparamètres sur l’approximation.

## Importation des bibliothèques

```{r}
library(tibble)
library(ggplot2)
library(gbm)
library(gridExtra)
```

## Génération du jeu de données

On considère un jeu de données (Xi,Yi), i=1,…,250, issu d’un modèle de régression :

$$Y_i = m(X_i)+\epsilon_i$$

où la vraie fonction est la fonction sinus.

```{r}
set.seed(4321)

x <- seq(-3*pi, 3*pi, by = 0.01)
y <- sin(x)

X <- runif(250, -3*pi, 3*pi)
Y <- sin(X) + rnorm(250, sd = 0.25)

df_train <- tibble(X = X, Y = Y)
df_true  <- tibble(X = x, Y = y)

ggplot(df_train, aes(X, Y)) +
  geom_point(alpha = 0.4) +
  geom_line(data = df_true, aes(X, Y), color = "blue", linewidth = 1) +
  xlab("") + ylab("") +
  ggtitle("Jeu de données : Sinus bruité")

```

## Ajustement d’un modèle GBM

1.  À l’aide de la fonction gbm du package gbm, construire un algorithme de gradient boosting pour prédire Y en fonction de X. On utilisera 50 000 itérations et une distribution gaussienne. On fixera la profondeur des arbres à 1 (stumps), le taux d’apprentissage (shrinkage) à 0.01, et bag.fraction = 1. Une validation croisée à 5 folds sera effectuée.
2.  Visualiser l’effet de la variable X sur la prédiction du modèle GBM avec la fonction plot.

```{r, eval=FALSE}
set.seed(4321)
L2boost <- gbm(
  formula = Y ~ ., ...
)


```

## Visualisation de l’estimateur aux différentes itérations

3.  Calculez les prédictions après 1, 1 000, 10 000, 20 000, 30 000 et 50 000 itérations.

4.  Tracez chaque prédiction en rouge sur les données d’apprentissage (points) et la fonction réelle (ligne bleue).

5.  Observez comment l’estimateur progresse vers la fonction cible.

```{r, eval=FALSE}
pred_1 <- predict(L2boost, ...)
pred_1000  <- predict(L2boost, ...)
pred_10000  <- predict(L2boost, ...)
pred_20000  <- predict(L2boost, ...)
pred_30000 <- predict(L2boost, ...)
pred_50000 <- predict(L2boost, ...)
```

## Sélection du nombre optimal d’arbres

6.  Identifiez le nombre optimal d’arbres (best_iter) à l’aide de la validation croisée intégrée.

7.  Affichez ce nombre et interprétez son rôle dans le compromis biais-variance.

```{r, eval=FALSE}
best_iter <- gbm.perf(...)

```

## Visualisation de l’estimateur optimal

8.  Calculez les prédictions du modèle en utilisant le nombre d’arbres optimal.

9.  Tracez ces prédictions en rouge sur les données d’apprentissage et la fonction réelle.

10. Observez la qualité finale de l’approximation.

```{r, eval=FALSE}
pred_opt <- predict(...)
plot_boost(...)

```
